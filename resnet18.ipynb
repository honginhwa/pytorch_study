{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qnzSWS8SzYEU",
        "F6jIrOBZzYEz",
        "lGLZo8R-zYE3",
        "DAsm_xUOzYE6",
        "mjUtVtQwzYE-",
        "3ItYnZwkzYFA",
        "5HRxEfu0zYFC",
        "Y8fzRXHIzYFE",
        "Vq8ly82YzYFH",
        "beEUk2kTzYFJ",
        "0Q4PHbv6zYFP",
        "UVRiLv7gzYFS",
        "94vTWDxvzYFV",
        "KK4Cc9kNzYFW",
        "r8j0M4hSzYFj",
        "qtHWdgPlzYFs",
        "wuMTiwKJzYFw",
        "e__D7E8qzYF4"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honginhwa/pytorch_study/blob/master/resnet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsNkMIY3zYDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import random as rand\n",
        "from random import *\n",
        "import os\n",
        "import cv2\n",
        "import copy\n",
        "import time\n",
        "from PIL import Image"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KrTzDwYzYDt",
        "colab_type": "text"
      },
      "source": [
        "# 첫 번째 시도(No Normalize & No Augmentation)\n",
        "### 데이터셋 정의 및 transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONq7mBpPzYDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgKd96cgzYDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"/content/drive/My Drive/Colab Notebooks/Labeld_Crop_Data\"\n",
        "trDsets = {x: dset.ImageFolder(os.path.join(data_dir, x), train_transforms[x]) for x in ['train', 'val']}\n",
        "trLoaders = {x: torch.utils.data.DataLoader(trDsets[x], batch_size=64, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "teDsets = dset.ImageFolder(os.path.join(data_dir,'test'), transform=test_transforms)\n",
        "teLoaders = torch.utils.data.DataLoader(teDsets, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0JVX7rnzYDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3d5caf3e-66d8-4c88-8b43-78674c4fd1e2"
      },
      "source": [
        "print(trDsets['train'].classes)\n",
        "print(teDsets.classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '3', '4', '5', '6', '7', '8', '9']\n",
            "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '3', '4', '5', '6', '7', '8', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE0cSue_05O8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f5a6b2a-c38d-4673-8449-85b51d695ef2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRoGc3P8zYD2",
        "colab_type": "text"
      },
      "source": [
        "여기서 틀린 부분을 알아챔..ㅠ  \n",
        "train class가 0, 1, 10 으로 시작하기 때문에, 이전에 만들어놨던 answer csv의 경우도 이 순서대로 했어야 했음.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SZ1ZZKVzYD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dce525b8-cd13-4e51-ec27-108f07aba82f"
      },
      "source": [
        "trDsets_sizes = {x: len(trDsets[x]) for x in ['train', 'val']}\n",
        "print(trDsets_sizes)\n",
        "class_names = trDsets['train'].classes\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train': 9936, 'val': 2503}\n",
            "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '3', '4', '5', '6', '7', '8', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhHscF1yzYD4",
        "colab_type": "text"
      },
      "source": [
        "### GPU 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjsg6w7hzYD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "97821691-88af-43ab-a614-3aa69b3f00a5"
      },
      "source": [
        "print(torch.cuda.is_available())     # GPU 사용 가능 여부\n",
        "print(torch.cuda.current_device())   # GPU 디바이스의 위치\n",
        "print(torch.cuda.device_count())     # 사용가능한 GPU 개수\n",
        "print(torch.cuda.get_device_name(0)) # GPU의 이름\n",
        "print(torch.cuda.device(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "0\n",
            "1\n",
            "Tesla T4\n",
            "<torch.cuda.device object at 0x7f143f4b2320>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HastesjlzYD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is0Fg-WAzYD_",
        "colab_type": "text"
      },
      "source": [
        "### 모델 생성 - ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBH5lxUTzYD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "273f251f-a6f8-443c-895f-d5addf00b9e7"
      },
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "num_ftrs = model.fc.in_features\n",
        "print(num_ftrs)\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=22, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtzE_3RyzYEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor=0.1, patience=11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31opDcEczYEE",
        "colab_type": "text"
      },
      "source": [
        "### 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay-hO0klzYEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=8):\n",
        "    \n",
        "    global_info = []\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) # state_dict() = 모델 불러오기 \n",
        "    best_acc = 0.0\n",
        "    early_stopping = EarlyStopping(patience=11, verbose=True)  # EarlyStopping = 11번에 큰 변화가 없다면 학습을 그만한다.\n",
        "    for epoch in range(num_epochs):\n",
        "        local_info = []\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)                                            #train 시작\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()                                   # list에 train이면 학습 val이면 평가 \n",
        "            else:\n",
        "                model.eval()                              \n",
        "                \n",
        "#                if epoch > 0:\n",
        "#                    scheduler.step(val_loss)\n",
        "                    \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in trLoaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()                              # 가중치 변화도 = 0 >> forward \n",
        "\n",
        "                # forward\n",
        "                # tensor는 인풋값이 디폴트는 그래디언트가 안들어가있는데 이 값을  \n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()                              # backward로 최신화로 스텝으로 가자 \n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)          # item() = tensor값을 scalar값으로 표기      \n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / trDsets_sizes[phase]\n",
        "            if phase == 'val':\n",
        "                val_loss = running_loss / trDsets_sizes['val']\n",
        "            epoch_acc = running_corrects.double() / trDsets_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "                local_info.append(epoch_loss)\n",
        "                ea = epoch_acc.cpu().numpy()\n",
        "                local_info.append(ea)\n",
        "            else:\n",
        "                local_info.append(epoch_loss)\n",
        "                ea = epoch_acc.cpu().numpy()\n",
        "                local_info.append(ea)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        lr_get = get_lr(optimizer)\n",
        "        print(\"Current learning rate : {:.8f}\".format(lr_get))\n",
        "        global_info.append(local_info)\n",
        "        \n",
        "        if phase =='val':\n",
        "            early_stopping(epoch_loss, model)\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "    \n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "        \n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5n2aXO3zYEG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "027ee545-3531-47da-ddf5-75a284cd3967"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "model_ft = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 0.5153 Acc: 0.8737\n",
            "val Loss: 0.9009 Acc: 0.6968\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (inf --> 0.900943).  Saving model ...\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 0.3251 Acc: 0.9168\n",
            "val Loss: 1.0228 Acc: 0.7004\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 0.2463 Acc: 0.9358\n",
            "val Loss: 0.5974 Acc: 0.8386\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.900943 --> 0.597357).  Saving model ...\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.2020 Acc: 0.9462\n",
            "val Loss: 0.3426 Acc: 0.9085\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.597357 --> 0.342562).  Saving model ...\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.1574 Acc: 0.9562\n",
            "val Loss: 2.4803 Acc: 0.5693\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.1308 Acc: 0.9662\n",
            "val Loss: 0.4910 Acc: 0.8466\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.1242 Acc: 0.9676\n",
            "val Loss: 0.2066 Acc: 0.9313\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.342562 --> 0.206586).  Saving model ...\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.1072 Acc: 0.9720\n",
            "val Loss: 0.3778 Acc: 0.8937\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.1126 Acc: 0.9677\n",
            "val Loss: 0.1525 Acc: 0.9509\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.206586 --> 0.152537).  Saving model ...\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.0853 Acc: 0.9770\n",
            "val Loss: 0.1315 Acc: 0.9581\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.152537 --> 0.131539).  Saving model ...\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.0793 Acc: 0.9782\n",
            "val Loss: 0.1698 Acc: 0.9465\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.0767 Acc: 0.9792\n",
            "val Loss: 0.1412 Acc: 0.9581\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.0684 Acc: 0.9820\n",
            "val Loss: 0.2304 Acc: 0.9349\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.0662 Acc: 0.9833\n",
            "val Loss: 0.4059 Acc: 0.8702\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.0573 Acc: 0.9856\n",
            "val Loss: 0.1226 Acc: 0.9652\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.131539 --> 0.122616).  Saving model ...\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.0583 Acc: 0.9837\n",
            "val Loss: 0.0933 Acc: 0.9704\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.122616 --> 0.093284).  Saving model ...\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.0530 Acc: 0.9861\n",
            "val Loss: 0.1371 Acc: 0.9592\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.0516 Acc: 0.9863\n",
            "val Loss: 0.1105 Acc: 0.9696\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.0482 Acc: 0.9869\n",
            "val Loss: 0.1020 Acc: 0.9728\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.0444 Acc: 0.9894\n",
            "val Loss: 0.1182 Acc: 0.9672\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.0476 Acc: 0.9874\n",
            "val Loss: 0.2056 Acc: 0.9497\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.0508 Acc: 0.9857\n",
            "val Loss: 0.1305 Acc: 0.9632\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.0427 Acc: 0.9877\n",
            "val Loss: 0.0988 Acc: 0.9712\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 7 out of 11\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.0599 Acc: 0.9830\n",
            "val Loss: 0.0855 Acc: 0.9804\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.093284 --> 0.085474).  Saving model ...\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.0452 Acc: 0.9863\n",
            "val Loss: 0.0711 Acc: 0.9796\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.085474 --> 0.071059).  Saving model ...\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.0362 Acc: 0.9900\n",
            "val Loss: 0.1234 Acc: 0.9684\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.0417 Acc: 0.9888\n",
            "val Loss: 0.0816 Acc: 0.9832\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.0358 Acc: 0.9904\n",
            "val Loss: 0.3545 Acc: 0.9017\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.0431 Acc: 0.9871\n",
            "val Loss: 0.0613 Acc: 0.9828\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.071059 --> 0.061348).  Saving model ...\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.0327 Acc: 0.9916\n",
            "val Loss: 0.0652 Acc: 0.9860\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.0288 Acc: 0.9921\n",
            "val Loss: 0.0664 Acc: 0.9872\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.0259 Acc: 0.9925\n",
            "val Loss: 0.0701 Acc: 0.9840\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.0319 Acc: 0.9912\n",
            "val Loss: 0.1253 Acc: 0.9716\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.0245 Acc: 0.9930\n",
            "val Loss: 0.0607 Acc: 0.9868\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.061348 --> 0.060677).  Saving model ...\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.0270 Acc: 0.9917\n",
            "val Loss: 0.0880 Acc: 0.9784\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.0246 Acc: 0.9937\n",
            "val Loss: 0.0584 Acc: 0.9876\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.060677 --> 0.058432).  Saving model ...\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.0254 Acc: 0.9934\n",
            "val Loss: 0.0920 Acc: 0.9796\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.0201 Acc: 0.9950\n",
            "val Loss: 0.1327 Acc: 0.9668\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.0284 Acc: 0.9916\n",
            "val Loss: 0.1402 Acc: 0.9624\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.0251 Acc: 0.9928\n",
            "val Loss: 0.1653 Acc: 0.9533\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.0236 Acc: 0.9912\n",
            "val Loss: 0.0722 Acc: 0.9820\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.0309 Acc: 0.9906\n",
            "val Loss: 0.1287 Acc: 0.9660\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.0259 Acc: 0.9929\n",
            "val Loss: 0.0545 Acc: 0.9892\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.058432 --> 0.054534).  Saving model ...\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.0170 Acc: 0.9952\n",
            "val Loss: 0.0513 Acc: 0.9920\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.054534 --> 0.051347).  Saving model ...\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.0160 Acc: 0.9953\n",
            "val Loss: 0.0572 Acc: 0.9880\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.0159 Acc: 0.9958\n",
            "val Loss: 0.0555 Acc: 0.9884\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.0221 Acc: 0.9933\n",
            "val Loss: 0.0560 Acc: 0.9880\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.0177 Acc: 0.9955\n",
            "val Loss: 0.0535 Acc: 0.9892\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.0144 Acc: 0.9961\n",
            "val Loss: 0.2367 Acc: 0.9509\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.0191 Acc: 0.9952\n",
            "val Loss: 0.0588 Acc: 0.9892\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.0150 Acc: 0.9959\n",
            "val Loss: 0.0588 Acc: 0.9892\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 7 out of 11\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.0230 Acc: 0.9932\n",
            "val Loss: 0.0501 Acc: 0.9912\n",
            "Current learning rate : 0.00100000\n",
            "Validation loss decreased (0.051347 --> 0.050091).  Saving model ...\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.0129 Acc: 0.9964\n",
            "val Loss: 0.0568 Acc: 0.9896\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.0156 Acc: 0.9955\n",
            "val Loss: 0.0709 Acc: 0.9848\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.0140 Acc: 0.9961\n",
            "val Loss: 0.0532 Acc: 0.9908\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.0148 Acc: 0.9962\n",
            "val Loss: 0.0778 Acc: 0.9816\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.0136 Acc: 0.9966\n",
            "val Loss: 0.0585 Acc: 0.9888\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.0170 Acc: 0.9962\n",
            "val Loss: 0.0541 Acc: 0.9908\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.0133 Acc: 0.9967\n",
            "val Loss: 0.0587 Acc: 0.9900\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 7 out of 11\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.0148 Acc: 0.9962\n",
            "val Loss: 0.0642 Acc: 0.9888\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 8 out of 11\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.0120 Acc: 0.9970\n",
            "val Loss: 0.0694 Acc: 0.9864\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 9 out of 11\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.0123 Acc: 0.9968\n",
            "val Loss: 0.0624 Acc: 0.9912\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 10 out of 11\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.0113 Acc: 0.9973\n",
            "val Loss: 0.0627 Acc: 0.9892\n",
            "Current learning rate : 0.00100000\n",
            "EarlyStopping counter: 11 out of 11\n",
            "Early stopping\n",
            "Training complete in 50m 38s\n",
            "Best val Acc: 0.992010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu6kMEcmzYEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model_ft, 'lotte_model_resnet18_v2.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHFq9jPKzYEN",
        "colab_type": "text"
      },
      "source": [
        "### 모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xge3eZj4zYEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "teDiter = iter(teLoaders) # 배치 1개씩 부른다.\n",
        "images, labels = teDiter.next()\n",
        "imshow(torchvision.utils.make_grid(images)) # 여러 이미지를 모아 하나의 이미지로 만들 수 잇다. \n",
        "print('GroundTruth: ', ' '.join('%5s' % class_names[labels[j]] for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiXepHCLzYEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testModel = torch.load('lotte_model_resnet18_v2.pt', map_location=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdM-qWQizYES",
        "colab_type": "text"
      },
      "source": [
        "#### 전체 테스트 결과"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evq7FJeIzYES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in teLoaders:\n",
        "        images, labels = data\n",
        "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
        "        \n",
        "        outputs = testModel(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnzSWS8SzYEU",
        "colab_type": "text"
      },
      "source": [
        "#### 클래스별 테스트 결과"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyvP4aWezYEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_classes = ['ID_gum', 'buttering', 'couque_coffee', 'chocopie', 'cidar', 'couque_white', 'coke', 'diget_ori', 'diget_choco', 'gumi_gumi', 'homerunball', 'jjolbyung_noodle', 'juicyfresh', 'jjolbyung_ori', 'spearmint', 'squid_peanut', 'samdasu', 'tuna', 'toreta', 'vita500', 'welchs', 'zec']\n",
        "tag_dict = dict()\n",
        "for i, label in enumerate(tag_classes):\n",
        "    tag_dict[i] = label\n",
        "\n",
        "print(tag_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xadgOUVnzYEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_correct = list(0 for i in range(len(class_names)))\n",
        "class_total = list(0 for i in range(len(class_names)))\n",
        "with torch.no_grad():\n",
        "    for data in teLoaders:\n",
        "        images, labels = data\n",
        "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
        "        \n",
        "        outputs = model_ft(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "\n",
        "        for i in range(c.size(0)):\n",
        "            label = labels[i]\n",
        "            class_correct[int(label.item())] += c[i].item()\n",
        "            class_total[int(label.item())] += 1\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "    print('Accuracy of %5s : %2d %%' % (tag_dict[int(class_names[i])], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2vUzIppzYEY",
        "colab_type": "text"
      },
      "source": [
        "# 두 번째 시도(Normalize & No Augmentation)\n",
        "### 데이터셋 정의 및 transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3jcryUrzYEZ",
        "colab_type": "text"
      },
      "source": [
        "normalized 된 것과 안된 것을 비교해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeloDuzGzYEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testPath = \"/content/drive/My Drive/Colab Notebooks/Resize_Crop_Data_2\"\n",
        "testDir = dset.ImageFolder(os.path.join(testPath, 'train'))\n",
        "numList = [i for i in range(len(testDir))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsSGgiv_zYEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_augmented_images(customTransform):\n",
        "    fig, subplots = plt.subplots(2, 5, figsize=(13, 6))\n",
        "    \n",
        "    for i in range(5):\n",
        "        axi1 = subplots.flat[i]\n",
        "        axi2 = subplots.flat[i+5]\n",
        "\n",
        "        ori_img = testDir[randSample[i]][0]\n",
        "        trf_img = customTransform(ori_img)\n",
        "\n",
        "        axi1.imshow(ori_img)\n",
        "        axi2.imshow(transforms.functional.to_pil_image(trf_img))\n",
        "        axi1.set_title('original')\n",
        "        axi2.set_title('transformed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g203T3xzYEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "norm1 = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "display_augmented_images(norm1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN0zThSrzYEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWBVbhTUzYEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"/content/drive/My Drive/Colab Notebooks/Labeld_Crop_Data\"\n",
        "trDsets = {x: dset.ImageFolder(os.path.join(data_dir, x), train_transforms[x]) for x in ['train', 'val']}\n",
        "trLoaders = {x: torch.utils.data.DataLoader(trDsets[x], batch_size=64, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "teDsets = dset.ImageFolder(os.path.join(data_dir, 'test'), transform=test_transforms)\n",
        "teLoaders = torch.utils.data.DataLoader(teDsets, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ARv_3SQzYEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trDsets_sizes = {x: len(trDsets[x]) for x in ['train', 'val']}\n",
        "class_names = trDsets['train'].classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB5nmy9xzYEm",
        "colab_type": "text"
      },
      "source": [
        "### 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DckXNLIzYEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device2 = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ookfLsI6zYEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = models.resnet18(pretrained=False)\n",
        "num_ftrs = model2.fc.in_features\n",
        "model2.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "model2 = model2.to(device2)\n",
        "print(model2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTnyFTdkzYEu",
        "colab_type": "text"
      },
      "source": [
        "위에서 학습할 때, patience가 10인 상태에서 더 좋은 결과를 냈기 때문에 조금 늘려주자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2PDKuGvzYEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion2 = nn.CrossEntropyLoss()\n",
        "optimizer_ft2 = optim.SGD(model2.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler2 = lr_scheduler.ReduceLROnPlateau(optimizer_ft2, factor=0.1, patience=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaVCSlfKzYEw",
        "colab_type": "text"
      },
      "source": [
        "### 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR92SjGrzYEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "model_ft2 = train_model(model2, criterion2, optimizer_ft2, exp_lr_scheduler2, num_epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJvqeWU1zYEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model_ft2, 'lotte_model_resnet18_v3.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6jIrOBZzYEz",
        "colab_type": "text"
      },
      "source": [
        "### 모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngHM-5HhzYE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testModel2 = torch.load('lotte_model_resnet18_v3.pt', map_location=device2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DauQYTZazYE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in teLoaders:\n",
        "        images, labels = data\n",
        "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
        "        \n",
        "        outputs = testModel2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGLZo8R-zYE3",
        "colab_type": "text"
      },
      "source": [
        "#### 클래스별 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaVvY6mxzYE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_correct = list(0 for i in range(len(class_names)))\n",
        "class_total = list(0 for i in range(len(class_names)))\n",
        "with torch.no_grad():\n",
        "    for data in teLoaders:\n",
        "        images, labels = data\n",
        "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
        "        \n",
        "        outputs = model_ft2(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "\n",
        "        for i in range(c.size(0)):\n",
        "            label = labels[i]\n",
        "            class_correct[int(label.item())] += c[i].item()\n",
        "            class_total[int(label.item())] += 1\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "    print('Accuracy of %5s : %2d %%' % (tag_dict[int(class_names[i])], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQLb9OprzYE5",
        "colab_type": "text"
      },
      "source": [
        "# 세 번째 시도(No Normalize & Augmentation)\n",
        "### 데이터셋 정의 및 transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBrVTIPuzYE5",
        "colab_type": "text"
      },
      "source": [
        "필요한 augmentation 기법만 골라서 사용해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liIwupzuzYE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_augmented_images(customTransform):\n",
        "    fig, subplots = plt.subplots(2, 5, figsize=(13, 6))\n",
        "    \n",
        "    for i in range(5):\n",
        "        axi1 = subplots.flat[i]\n",
        "        axi2 = subplots.flat[i+5]\n",
        "\n",
        "        ori_img = testDir[randSample[i]][0]\n",
        "        trf_img = customTransform(ori_img)\n",
        "\n",
        "        axi1.imshow(ori_img)\n",
        "        axi2.imshow(trf_img)\n",
        "        axi1.set_title('original')\n",
        "        axi2.set_title('transformed')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAsm_xUOzYE6",
        "colab_type": "text"
      },
      "source": [
        "#### Random Horizontal Flip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T8b8ESUzYE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "horiz_flip = transforms.RandomHorizontalFlip(p=1)\n",
        "display_augmented_images(horiz_flip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjUtVtQwzYE-",
        "colab_type": "text"
      },
      "source": [
        "#### Random Vertical Flip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmGWjgUgzYE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "vert_flip = transforms.RandomVerticalFlip(p=1)\n",
        "display_augmented_images(vert_flip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ItYnZwkzYFA",
        "colab_type": "text"
      },
      "source": [
        "#### Random Affine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNfhIlQ2zYFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "affine = transforms.RandomAffine(30)\n",
        "display_augmented_images(affine)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HRxEfu0zYFC",
        "colab_type": "text"
      },
      "source": [
        "#### Random Crop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdN8TbcfzYFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "crop = transforms.RandomCrop((200, 200))\n",
        "display_augmented_images(crop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8fzRXHIzYFE",
        "colab_type": "text"
      },
      "source": [
        "#### Random Perspective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvhlXN3LzYFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "perspect = transforms.RandomPerspective(p=1)\n",
        "display_augmented_images(perspect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq8ly82YzYFH",
        "colab_type": "text"
      },
      "source": [
        "#### Random Rotation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1q7HCeuzYFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "rot = transforms.RandomRotation(90)\n",
        "display_augmented_images(rot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beEUk2kTzYFJ",
        "colab_type": "text"
      },
      "source": [
        "#### Color Jitter(Brightness)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWas8dzdzYFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "color = transforms.ColorJitter(brightness=(0.5, 1.5))\n",
        "display_augmented_images(color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q4PHbv6zYFP",
        "colab_type": "text"
      },
      "source": [
        "#### Color Jitter(Contrast)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-_t4PUBzYFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "color = transforms.ColorJitter(contrast=(0.5, 3))\n",
        "display_augmented_images(color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVRiLv7gzYFS",
        "colab_type": "text"
      },
      "source": [
        "#### Color Jitter(Saturation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5w11rQyzYFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "color = transforms.ColorJitter(saturation=(0.5, 3))\n",
        "display_augmented_images(color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94vTWDxvzYFV",
        "colab_type": "text"
      },
      "source": [
        "#### Color Jitter(Hue)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdhILZ0czYFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "color = transforms.ColorJitter(hue=(-0.5, 0.5))\n",
        "display_augmented_images(color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4Cc9kNzYFW",
        "colab_type": "text"
      },
      "source": [
        "#### Pad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI0zHnglzYFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randSample = rand.sample(numList, 5)\n",
        "pad = transforms.Pad(padding=10)\n",
        "display_augmented_images(pad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TVUAjZnzYFY",
        "colab_type": "text"
      },
      "source": [
        "꽤 쓸만한 augmentation 기법  \n",
        "- Random Horizontal Flip\n",
        "- Random Vertical Flip\n",
        "- Color Jitter(Brightness)\n",
        "- Color Jitter(Contrast)\n",
        "- Color Jitter(Saturation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LqM2PngzYFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 위의 기법들을 확률적으로 선택해서 적용하는 transforms를 만들어보자.\n",
        "train_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomChoice([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.ColorJitter(brightness=(0.5, 1.5)),\n",
        "            transforms.ColorJitter(contrast=(0.5, 3)),\n",
        "            transforms.ColorJitter(saturation=(0.5, 3)),\n",
        "        ]),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "}\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AilYXjyhzYFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"../Data_Set/Labeld_Crop_Data/\"\n",
        "trDsets = {x: dset.ImageFolder(os.path.join(data_dir, x), train_transforms[x]) for x in ['train', 'val']}\n",
        "trLoaders = {x: torch.utils.data.DataLoader(trDsets[x], batch_size=64, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "teDsets = dset.ImageFolder(os.path.join(data_dir, 'test'), transform=test_transforms)\n",
        "teLoaders = torch.utils.data.DataLoader(teDsets, batch_size=64, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVj8G7dczYFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trDsets_sizes = {x: len(trDsets[x]) for x in ['train', 'val']}\n",
        "class_names = trDsets['train'].classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8j0M4hSzYFj",
        "colab_type": "text"
      },
      "source": [
        "### 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbIrpeEczYFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device3 = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Djd-FIdzYFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3 = models.resnet18(pretrained=False)\n",
        "num_ftrs = model3.fc.in_features\n",
        "model3.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "model3 = model3.to(device3)\n",
        "print(model3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiH2VBrlzYFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion3 = nn.CrossEntropyLoss()\n",
        "optimizer_ft3 = optim.SGD(model3.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler3 = lr_scheduler.ReduceLROnPlateau(optimizer_ft3, factor=0.1, patience=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtHWdgPlzYFs",
        "colab_type": "text"
      },
      "source": [
        "### 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpGHH8JszYFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "model_ft3 = train_model(model3, criterion3, optimizer_ft3, exp_lr_scheduler3, num_epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuMTiwKJzYFw",
        "colab_type": "text"
      },
      "source": [
        "### 모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FznHB1c8zYFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model_ft3, 'lotte_model_resnet18_v4.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRECwAFXzYF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testModel3 = torch.load('lotte_model_resnet18_v4.pt', map_location=device3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQShn4n-zYF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in teLoaders:\n",
        "        images, labels = data\n",
        "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
        "        \n",
        "        outputs = testModel3(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e__D7E8qzYF4",
        "colab_type": "text"
      },
      "source": [
        "#### 클래스별 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEB_sj6TzYF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_correct = list(0 for i in range(len(class_names)))\n",
        "class_total = list(0 for i in range(len(class_names)))\n",
        "with torch.no_grad():\n",
        "    for data in teLoaders:\n",
        "        images, labels = data\n",
        "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
        "        \n",
        "        outputs = testModel3(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "\n",
        "        for i in range(c.size(0)):\n",
        "            label = labels[i]\n",
        "            class_correct[int(label.item())] += c[i].item()\n",
        "            class_total[int(label.item())] += 1\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "    print('Accuracy of %5s : %2d %%' % (tag_dict[int(class_names[i])], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}